{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267c62ab-6b45-4390-91f7-381058141ed0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Doubly-sparse PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833ae171-dc01-41de-aa95-ae787dd4b9d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Problem statement\n",
    "\n",
    "This method is trying to solve the biclustering problem: given an $n \\times p$ matrix with data X (where the data represents gene expression, e.g. $n$ is the number of  patients and $p$ is the number of genes with their expression measured), we are trying to find a principal component such, that it represents a subset of genes and a subset of patients for which the sum of square distances from data points to this principal component is minimal.\n",
    "\n",
    "$X = \\begin{pmatrix} x_{1,1} && x_{1,2} && ... && x_{1,p} \\\\ ... && ... && ... && ... \\\\ x_{n,1} && x_{n,2} && ... && x_{n,p} \\end{pmatrix}$\n",
    "\n",
    "Let ${\\bf v} = (v_1, v_2, ..., v_p)^T$ be the first principal component vector, we are looking for. We want a $k$-sparse version of this vector (i.e. we want its $L_0$ norm to be $k$), so that only $k$ coordinates of it are non-zero. Unfortunately, with $L_0$ restriction, the problem is NP-hard, so instead of using $L_0$ norm, we will use $L_1$ norm as its proxy.\n",
    "\n",
    "We would also consider only a subset of data points projections on this principal component. Again, to select the member data point of this subset, we need to apply $L_0$ norm as a restriction, but instead we will go for $L_1$ norm. The mask, meant to select just a subset of the points, would be represented by a vector ${\\bf u} = (u_1, u_2, ..., u_n)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd247b0c-3f91-4c17-9988-59453adc6615",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Thus, the function, we are seeking to optimize is:\n",
    "\n",
    "$f(u, v) = \\begin{pmatrix} v_1 && v_2 && v_3 && v_4 && v_5 \\end{pmatrix} \\begin{pmatrix} x_{1,1} && ... && x_{n,1} \\\\  x_{1,2} && ... && x_{n,2} \\\\ x_{1,3} && ... && x_{n,3} \\\\ ... && ... && ... \\\\ x_{1,p} && ... && x_{n,p} \\end{pmatrix} \\begin{pmatrix} u_1 && 0 && 0 \\\\ 0 && u_2 && 0 \\\\ 0 && 0 && u_n \\end{pmatrix} \\begin{pmatrix} x_{1,1} && x_{1,2} && x_{1,3} && ... && x_{1,p} \\\\ ... && ... && ... && ... && ... \\\\ x_{n,1} && x_{n,2} && x_{n,3} && ... && x_{n,p} \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ ... \\\\ v_p \\end{pmatrix} \\to \\min$\n",
    "\n",
    "Upon conditions:\n",
    "\n",
    "$\\sum \\limits_{i=1}^{p} | v_i | = k$\n",
    "\n",
    "$\\sum \\limits_{i=1}^{n} | u_i | = s$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555bf414-48f6-40d2-97b4-f8a8a018df49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Solution\n",
    "\n",
    "Re-write the function in scalar way:\n",
    "\n",
    "$f({\\bf u}, {\\bf v}) = \\begin{pmatrix} \\sum \\limits_{i=1}^{n} v_i x_{1,i} && ... && \\sum \\limits_{i=1}^{n} v_i x_{n,i} \\end{pmatrix} \\begin{pmatrix} u_1 && 0 && 0 \\\\ 0 && u_2 && 0 \\\\ 0 && 0 && u_n \\end{pmatrix} \\begin{pmatrix} \\sum \\limits_{i=1}^p x_{1,i} v_i \\\\ ... \\\\ \\sum \\limits_{i=1}^p x_{n,i} v_i \\end{pmatrix} = u_1 \\cdot (\\sum \\limits_{i=1}^p x_{1,i} v_i)^2 + u_2 \\cdot (\\sum \\limits_{i=1}^p x_{2,i} v_i)^2 + ... + u_n \\cdot (\\sum \\limits_{i=1}^p x_{n,i} v_i)^2 = \\sum \\limits_{j=1}^n u_j (\\sum \\limits_{i=1}^p x_{j,i} v_i)^2$\n",
    "\n",
    "Here we have $n$ variables $u_j$ and $p$ variables $v_i$. We need the partial derivatives of $f({\\bf u}, {\\bf v})$ on those variables to equal zero, and also two linear constraints to hold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7358ddbe-2b63-4b21-817e-229134d6bc2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "$\\frac{\\partial f}{\\partial u_j} = (\\sum \\limits_{i=1}^p x_{j,i} v_i)^2$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial v_i} = 2 u_1 (\\sum \\limits_{t=1}^p x_{1,t} v_t) x_{1,i} + 2 u_2 (\\sum \\limits_{t=1}^p x_{2,t} v_t) x_{2,i} + ... + 2 u_n (\\sum \\limits_{t=1}^p x_{n,t} v_t) x_{n,i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb37be65-c1f9-4cb3-affe-6ec9a5765f6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We have to add constraints with Lagrange multipliers to these equations (TODO: I discarded absolute values here! Fix it!):\n",
    "\n",
    "$ -\\lambda_1 \\sum \\limits_{i=1}^p v_i = -\\lambda_1 k$\n",
    "\n",
    "$ -\\lambda_2 \\sum \\limits_{j=1}^p u_j = -\\lambda_2 s$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f0bf1-d9d7-44ee-9ff7-b99ee0e260b3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Hence, we get a set of $p + n + 2$ equations:\n",
    "\n",
    "$\\frac{\\partial (f + constraints)}{\\partial u_j} = (\\sum \\limits_{i=1}^p x_{j,i} v_i)^2 - \\lambda_2 \\cdot sign(u_j) = 0$\n",
    "\n",
    "$\\frac{\\partial (f + constraints)}{\\partial v_i} = 2 u_1 (\\sum \\limits_{t=1}^p x_{1,t} v_t) x_{1,i} + 2 u_2 (\\sum \\limits_{t=1}^p x_{2,t} v_t) x_{2,i} + ... + 2 u_n (\\sum \\limits_{t=1}^p x_{n,t} v_t) x_{n,i} - \\lambda_1 \\cdot sign(v_i) = 0$\n",
    "\n",
    "$\\sum \\limits_{i=1}^p | v_i | = k$\n",
    "\n",
    "$\\sum \\limits_{j=1}^p | u_j |= s$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aab0f9b-352f-42c1-913b-479cf6123c59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "$\n",
    "\\begin{equation}\n",
    "  \\begin{cases}\n",
    "    (\\sum \\limits_{i=1}^p x_{j,i} v_i)^2 = \\lambda_2 \\cdot sign(u_j) \\\\\n",
    "    2 u_1 (\\sum \\limits_{t=1}^p x_{1,t} v_t) x_{1,i} + 2 u_2 (\\sum \\limits_{t=1}^p x_{2,t} v_t) x_{2,i} + ... + 2 u_n (\\sum \\limits_{t=1}^p x_{n,t} v_t) x_{n,i} = \\lambda_1 \\cdot sign(v_i) \\\\\n",
    "    \\sum \\limits_{i=1}^p | v_i | = k \\\\\n",
    "    \\sum \\limits_{j=1}^p | u_j | = s \\\\\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14cb76ef-78bf-48da-bf65-c7a38324c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubly_sparse_pca_sum_of_squared_distances(X, v, u):\n",
    "    \"\"\"Returns the square sum of distances from a subset of datapoints X, masked by vector u,\n",
    "    to the first sparse principal component axis, determined by vector v.\n",
    "    \n",
    "    :param 2D-ndarray X: n-by-p (n rows, p columns) data matrix, where n \n",
    "      correspond to data vectors (e.g. RNA-seq'ed human) and p correspond to\n",
    "      predictors (e.g. gene expressions);\n",
    "    :param ndarray v: L1-norm-limited first principal component vector; it is\n",
    "      a sparse p-vector that selects a subset of predictors to be non-zero,\n",
    "      while the remaining predictors are zero;\n",
    "    :param ndarray u: L1-norm-limited mask that selects a subset of datapoints,\n",
    "      for which we minimize the sum of square distances to the first principal\n",
    "      component vector; it is a sparse n-vector\n",
    "    :raise: ValueError if len(v) != len(X) or if len(u) != len(X[0]) or if X/v/u are empty\n",
    "    :return: sum of squared distances from the selected subset of vectors to\n",
    "      the first principal component\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    if len(X) == 0:\n",
    "        raise ValueError(f\"len(X) == 0\")\n",
    "    \n",
    "    if len(X[0]) == 0:\n",
    "        raise ValueError(f\"len(X[0]) == 0\")\n",
    "    \n",
    "    if len(v) == len(X):\n",
    "        raise ValueError(f\"len(v) != len(X): len(v) = {len(v)}, len(X) = {len(X)}\")\n",
    "        \n",
    "    if len(u) == len(X[0]):\n",
    "        raise ValueError(f\"len(u) != len(X[0]): len(u) = {len(u)}, len(X[0]) = {len(X[0])}\")\n",
    "        \n",
    "    sum: float = 0\n",
    "    for j, u_j in enumerate(u):\n",
    "        squared_distance: float = 0\n",
    "        for i, v_i in enumerate(v):\n",
    "            squared_distance += X[j][i] * v_i  # TODO: refactor as a \n",
    "        sum += squared_distance\n",
    "\n",
    "    return sum\n",
    "\n",
    "\n",
    "def double_sparse_pca_sum_of_sqared_distances_derivatives():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f40b8a8-6c76-4bb7-af09-1b0af04a6daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.optimize import LinearConstraint, minimize\n",
    "\n",
    "\n",
    "linear_constraint = LinearConstraint([[1, 2], [2, 1]], [-np.inf, 1], [1, 1])\n",
    "\n",
    "res = minimize(\n",
    "    doubly_sparse_pca_sum_of_squared_distances, \n",
    "    X,\n",
    "    method='SLSQP',\n",
    "    jac=rosen_der,\n",
    "    constraints=[inequality_constraints], \n",
    "    options={'ftol': 1e-9, 'disp': True},\n",
    "    bounds=bounds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ae762-2af7-4c0b-b350-50f31b1f9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
